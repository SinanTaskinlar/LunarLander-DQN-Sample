{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "#import os",
   "id": "8fcbb5a54f2e349a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import random\n",
    "from collections import deque\n",
    "from multiprocessing import Process\n",
    "from multiprocessing import Manager\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ],
   "id": "2d66e3c7938ee20a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "def plot_all_algorithms(dqn_rewards, ppo_rewards, a3c_rewards):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(dqn_rewards, label=\"DQN\", color=\"blue\")\n",
    "    plt.plot(ppo_rewards, label=\"PPO\", color=\"green\")\n",
    "    plt.plot(a3c_rewards, label=\"A3C\", color=\"yellow\")\n",
    "    plt.xlabel(\"Deneme Sayısı\")\n",
    "    plt.ylabel(\"Ödül Değeri\")\n",
    "    plt.title(\"LunarLander Ortamında DQN-PPO-A3C Algoritma Karşılaştırması\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_model(model, model_path):\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved at {model_path}\")\n",
    "\n",
    "\n",
    "def load_model(model, model_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()  # Modeli değerlendirme (inference) moduna al\n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "\n",
    "\n",
    "# Utility class for wandb logging and reward plotting\n",
    "class PlotAndLog:\n",
    "    def __init__(self, algorithm_name):\n",
    "        self.algorithm_name = algorithm_name\n",
    "        # wandb.init(project=\"RL-Algorithms\", name=self.algorithm_name, config={})\n",
    "\n",
    "    @staticmethod\n",
    "    def log(episode, reward, **kwargs):\n",
    "        log_data = {\"Episode\": episode, \"Reward\": reward}\n",
    "        log_data.update(kwargs)\n",
    "        # wandb.log(log_data)\n",
    "\n",
    "    def plot_rewards(self, rewards):\n",
    "        print(f\"Plotting rewards for {self.algorithm_name}.\")\n",
    "        print(rewards)\n",
    "\n",
    "\n",
    "# Define DQN model\n",
    "class DQNModel(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_layers=(64, 64)):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        input_dim = state_size\n",
    "        for layer_size in hidden_layers:\n",
    "            layers.append(nn.Linear(input_dim, layer_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = layer_size\n",
    "        layers.append(nn.Linear(input_dim, action_size))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "# Trainer for DQN\n",
    "class DQNTrainer:\n",
    "    def __init__(self, env, state_size, action_size, config):\n",
    "        self.env = env\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.config = config\n",
    "        self.model = DQNModel(state_size, action_size).to(self.device)\n",
    "        self.target_model = DQNModel(state_size, action_size).to(self.device)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=config['lr'])\n",
    "        self.memory = deque(maxlen=config.get('memory_size', 10000))\n",
    "        self.epsilon = config.get('epsilon_start', 1.0)\n",
    "        self.plot_and_log = PlotAndLog(\"DQN\")\n",
    "\n",
    "    def train(self, max_episodes=1000):\n",
    "        rewards = []\n",
    "\n",
    "        for episode in range(max_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            state = torch.FloatTensor(state).to(self.device)\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                action = self._select_action(state)\n",
    "                next_state, reward, done, truncated, _ = self.env.step(action)\n",
    "                done = done or truncated\n",
    "                next_state = torch.FloatTensor(next_state).to(self.device)\n",
    "                self.memory.append((state, action, reward, next_state, done))\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "                if len(self.memory) >= self.config['batch_size']:\n",
    "                    self._learn()\n",
    "\n",
    "            self.epsilon = max(\n",
    "                self.config.get('epsilon_end', 0.01),\n",
    "                self.epsilon * self.config.get('epsilon_decay', 0.995)\n",
    "            )\n",
    "\n",
    "            rewards.append(total_reward)\n",
    "            #self.plot_and_log.log(episode, total_reward)\n",
    "\n",
    "            if episode % self.config.get('save_freq', 1000) == 0:\n",
    "                save_model(self.model, f\"dqn_model_{episode}.pth\")\n",
    "\n",
    "            print(f\"Episode {episode}, Reward: {total_reward}\")\n",
    "\n",
    "            if episode % self.config.get('target_update_freq', 10) == 0:\n",
    "                self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "        self.plot_and_log.plot_rewards(rewards)\n",
    "        return rewards\n",
    "\n",
    "    def _select_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.model(state)\n",
    "                return torch.argmax(q_values).item()\n",
    "\n",
    "    def _learn(self):\n",
    "        batch = random.sample(self.memory, self.config['batch_size'])\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.stack(states)\n",
    "        next_states = torch.stack(next_states)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "\n",
    "        current_q = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        next_q = self.target_model(next_states).max(1)[0]\n",
    "        target_q = rewards + self.config['gamma'] * next_q * (1 - dones)\n",
    "\n",
    "        loss = nn.MSELoss()(current_q, target_q.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "# Define PPO model\n",
    "class PPOModel(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_layers=(64, 64)):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        input_dim = state_size\n",
    "        for layer_size in hidden_layers:\n",
    "            layers.append(nn.Linear(input_dim, layer_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = layer_size\n",
    "        self.shared_layers = nn.Sequential(*layers)\n",
    "        self.policy_head = nn.Linear(input_dim, action_size)\n",
    "        self.value_head = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared = self.shared_layers(x)\n",
    "        policy = torch.softmax(self.policy_head(shared), dim=-1)\n",
    "        value = self.value_head(shared)\n",
    "        return policy, value\n",
    "\n",
    "\n",
    "# Trainer for PPO\n",
    "class PPOTrainer:\n",
    "    def __init__(self, env, state_size, action_size, config):\n",
    "        self.env = env\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.config = config\n",
    "        self.model = PPOModel(state_size, action_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=config['lr'])\n",
    "        self.plot_and_log = PlotAndLog(\"PPO\")\n",
    "\n",
    "    def train(self, max_episodes=1000):\n",
    "        rewards = []\n",
    "\n",
    "        for episode in range(max_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            state = torch.FloatTensor(state).to(self.device)\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                policy, value = self.model(state)\n",
    "                action = torch.distributions.Categorical(policy).sample().item()\n",
    "                next_state, reward, done, truncated, _ = self.env.step(action)\n",
    "                done = done or truncated\n",
    "                next_state = torch.FloatTensor(next_state).to(self.device)\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "\n",
    "            rewards.append(total_reward)\n",
    "            # self.plot_and_log.log(episode, total_reward)\n",
    "\n",
    "            if episode % self.config.get('save_freq', 1000) == 0:\n",
    "                save_model(self.model, f\"ppo_model_{episode}.pth\")\n",
    "\n",
    "            print(f\"Episode {episode}, Reward: {total_reward}\")\n",
    "\n",
    "        self.plot_and_log.plot_rewards(rewards)\n",
    "        return rewards\n",
    "\n",
    "\n",
    "# Define A3C Model\n",
    "class A3CModel(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_layers=(128, 128)):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        input_dim = state_size\n",
    "        for layer_size in hidden_layers:\n",
    "            layers.append(nn.Linear(input_dim, layer_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = layer_size\n",
    "        self.shared_layers = nn.Sequential(*layers)\n",
    "        self.policy_head = nn.Linear(input_dim, action_size)\n",
    "        self.value_head = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared = self.shared_layers(x)\n",
    "        policy = torch.softmax(self.policy_head(shared), dim=-1)\n",
    "        value = self.value_head(shared)\n",
    "        return policy, value\n",
    "\n",
    "\n",
    "# Worker for A3C training\n",
    "class A3CWorker:\n",
    "    def __init__(self, global_model, optimizer, env_name, config, worker_id, reward_list):\n",
    "        self.global_model = global_model\n",
    "        self.optimizer = optimizer\n",
    "        self.env = gym.make(env_name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.config = config\n",
    "        self.worker_id = worker_id\n",
    "        self.local_model = A3CModel(\n",
    "            self.env.observation_space.shape[0],\n",
    "            self.env.action_space.n\n",
    "        ).to(self.device)\n",
    "        self.reward_list = reward_list\n",
    "\n",
    "    def train(self):\n",
    "        for episode in range(self.config['max_episodes']):\n",
    "            state, _ = self.env.reset()\n",
    "            state = torch.FloatTensor(state).to(self.device)\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            log_probs = []\n",
    "            values = []\n",
    "            rewards = []\n",
    "\n",
    "            while not done:\n",
    "                policy, value = self.local_model(state)\n",
    "                action_dist = Categorical(policy)\n",
    "                action = action_dist.sample()\n",
    "\n",
    "                log_probs.append(action_dist.log_prob(action))\n",
    "                values.append(value)\n",
    "\n",
    "                next_state, reward, done, truncated, _ = self.env.step(action.item())\n",
    "                done = done or truncated\n",
    "                rewards.append(reward)\n",
    "                total_reward += reward\n",
    "                state = torch.FloatTensor(next_state).to(self.device)\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # Store rewards for this worker in shared reward list\n",
    "            self.reward_list.append(total_reward)\n",
    "\n",
    "            # Compute advantages and update global model\n",
    "            self._update_global_model(rewards, log_probs, values)\n",
    "\n",
    "            print(f\"Worker {self.worker_id} | Episode {episode} | Reward: {total_reward}\")\n",
    "\n",
    "    def _update_global_model(self, rewards, log_probs, values):\n",
    "        # Compute returns and advantages\n",
    "        returns = []\n",
    "        g = 0\n",
    "        for r in reversed(rewards):\n",
    "            g = r + self.config['gamma'] * g\n",
    "            returns.insert(0, g)\n",
    "\n",
    "        returns = torch.FloatTensor(returns).to(self.device)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantages = returns - values.detach()\n",
    "\n",
    "        # Policy loss and value loss\n",
    "        policy_loss = -(log_probs * advantages).mean()\n",
    "        value_loss = nn.MSELoss()(values, returns)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss = policy_loss + self.config['value_loss_coef'] * value_loss\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for global_param, local_param in zip(self.global_model.parameters(), self.local_model.parameters()):\n",
    "            global_param._grad = local_param.grad\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Synchronize local model with global model\n",
    "        self.local_model.load_state_dict(self.global_model.state_dict())\n",
    "\n",
    "\n",
    "# Main A3C Training Process\n",
    "class A3CTrainer:\n",
    "    def __init__(self, env, state_size, action_size, config):\n",
    "        self.env = env\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.config = config\n",
    "        self.global_model = A3CModel(state_size, action_size).to(\n",
    "            torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "        self.global_model.share_memory()\n",
    "        self.optimizer = optim.Adam(self.global_model.parameters(), lr=config['lr'])\n",
    "\n",
    "    def train(self):\n",
    "        manager = Manager()\n",
    "        reward_list = manager.list()  # Paylaşımlı ödül listesi. Plotting için bu şekilde yapıldı\n",
    "        processes = []\n",
    "        for worker_id in range(self.config['num_workers']):\n",
    "            worker = A3CWorker(self.global_model, self.optimizer, self.env, self.config, worker_id, reward_list)\n",
    "            process = Process(target=worker.train)\n",
    "            process.start()\n",
    "            processes.append(process)\n",
    "\n",
    "        for process in processes:\n",
    "            process.join()\n",
    "\n",
    "        save_model(self.global_model, f\"a3c_model.pth\")\n",
    "        print(\"A3C model saved.\")\n",
    "\n",
    "        return reward_list\n",
    "\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    max_episodes = 5\n",
    "    environment_name = \"LunarLander-v3\"\n",
    "    render_mode = \"human\"\n",
    "    # render_mode = None\n",
    "\n",
    "    # A3C Configuration\n",
    "    a3c_env = gym.make(environment_name, render_mode=render_mode)\n",
    "    a3c_config = {\n",
    "        'lr': 1e-4,  #0.0001\n",
    "        'gamma': 0.99,\n",
    "        'value_loss_coef': 0.5,\n",
    "        'num_workers': 4,\n",
    "        'max_episodes': max_episodes\n",
    "    }\n",
    "    a3c_trainer = A3CTrainer(environment_name, a3c_env.observation_space.shape[0], a3c_env.action_space.n, a3c_config)\n",
    "    a3c_rewards = a3c_trainer.train()\n",
    "\n",
    "    # DQN Configuration\n",
    "    dqn_env = gym.make(environment_name, render_mode=render_mode)\n",
    "    dqn_config = {\n",
    "        'lr': 1e-3,  #0.0001\n",
    "        'gamma': 0.99,\n",
    "        'epsilon_start': 1.0,\n",
    "        'epsilon_end': 0.05,\n",
    "        'epsilon_decay': 0.995,\n",
    "        'batch_size': 128,\n",
    "        'memory_size': 10000,\n",
    "        'target_update_freq': 10\n",
    "    }\n",
    "    dqn_trainer = DQNTrainer(dqn_env, dqn_env.observation_space.shape[0], dqn_env.action_space.n, dqn_config)\n",
    "    dqn_rewards = dqn_trainer.train(max_episodes=max_episodes)\n",
    "\n",
    "    # PPO Configuration\n",
    "    ppo_env = gym.make(environment_name, render_mode=render_mode)\n",
    "    ppo_config = {\n",
    "        'lr': 3e-4,  #0.0003\n",
    "        'gamma': 0.99\n",
    "    }\n",
    "    ppo_trainer = PPOTrainer(ppo_env, ppo_env.observation_space.shape[0], ppo_env.action_space.n, ppo_config)\n",
    "    ppo_rewards = ppo_trainer.train(max_episodes=max_episodes)\n",
    "\n",
    "    # Kaydedilen modeli istersek yüklemek\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    dqn_model = DQNModel(a3c_env.observation_space.shape[0], a3c_env.action_space.n).to(device)\n",
    "    load_model(dqn_model, \"dqn_model_500.pth\")\n",
    "    ppo_model = PPOModel(a3c_env.observation_space.shape[0], a3c_env.action_space.n).to(device)\n",
    "    load_model(ppo_model, \"ppo_model_500.pth\")\n",
    "    a3c_model = A3CModel(a3c_env.observation_space.shape[0], a3c_env.action_space.n).to(device)\n",
    "    load_model(a3c_model, \"a3c_model.pth\")\n",
    "\n",
    "    # Plot all algorithms\n",
    "    plot_all_algorithms(dqn_rewards, ppo_rewards, a3c_rewards)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "242fa651845d6f74"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
